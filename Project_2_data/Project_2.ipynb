{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form(X, Y, lambda_factor):\n",
    "    \"\"\"\n",
    "    Computes the closed form solution of linear regression with L2 regularization\n",
    "\n",
    "    Args:\n",
    "        X - (n, d + 1) NumPy array (n datapoints each with d features plus the bias feature in the first dimension)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "    Returns:\n",
    "        theta - (d + 1, ) NumPy array containing the weights of linear regression. Note that theta[0]\n",
    "        represents the y-axis intercept of the model and therefore X[0] = 1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    iden_mat = np.identity(X.shape[1])\n",
    "    inversible = np.matmul(X.T, X) - (-lambda_factor) * iden_mat\n",
    "    inverse = np.linalg.inv(inversible)\n",
    "    return (np.matmul(inverse, np.matmul(X.T,Y)))\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_regression_on_MNIST(lambda_factor=1):\n",
    "    \"\"\"\n",
    "    Trains linear regression, classifies test data, computes test error on test set\n",
    "\n",
    "    Returns:\n",
    "        Final test error\n",
    "    \"\"\"\n",
    "    train_x, train_y, test_x, test_y = get_MNIST_data()\n",
    "    train_x_bias = np.hstack([np.ones([train_x.shape[0], 1]), train_x])\n",
    "    test_x_bias = np.hstack([np.ones([test_x.shape[0], 1]), test_x])\n",
    "    theta = closed_form(train_x_bias, train_y, lambda_factor)\n",
    "    test_error = compute_test_error_linear(test_x_bias, test_y, theta)\n",
    "    return test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.08306065, 0.40367245],\n",
    " [0.73721212, 0.1446548 ],\n",
    " [0.37854212, 0.91848412],\n",
    " [0.34735579, 0.92967347],\n",
    " [0.58275768, 0.02878153],\n",
    " [0.11577078, 0.3793189 ],\n",
    " [0.49626992, 0.03433923],\n",
    " [0.93620684, 0.81767935],\n",
    " [0.50209705, 0.76204725],\n",
    " [0.71287984, 0.12056536],\n",
    " [0.8482472,  0.63871636],\n",
    " [0.89746806, 0.60944071],\n",
    " [0.80312304, 0.39808585],\n",
    " [0.17187538, 0.96388742],\n",
    " [0.36947729, 0.70868798],\n",
    " [0.54650151, 0.92196146],\n",
    " [0.04545365, 0.32840425],\n",
    " [0.20310255, 0.77202396]])\n",
    "Y = np.array([0.47448298, 0.24419894, 0.02347939, 0.69281014, 0.38118102, 0.30887115,\n",
    " 0.04431094, 0.49541117, 0.9945525,  0.49814587, 0.981581,   0.20000342,\n",
    " 0.02283484, 0.35575137, 0.03563995, 0.27161267, 0.13116319, 0.87779631])\n",
    "lambda_factor = 0.7052750700565534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25953114, 0.39594302])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closed_form(X, Y, lambda_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(X, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes, for each datapoint X[i], the probability that X[i] is labeled as j\n",
    "    for j = 0, 1, ..., k-1\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our model for label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "    Returns:\n",
    "        H - (k, n) NumPy array, where each entry H[j][i] is the probability that X[i] is labeled as j\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "#     print(\"X: \", X, \"\\ntheta: \", theta, \"\\ntemp: \", temp_parameter)\n",
    "    c_param = np.dot(theta, X.T) /temp_parameter\n",
    "    c = np.max(c_param, axis = 0)\n",
    "    H = np.exp(c_param - c)\n",
    "    H = H/np.sum(H, axis = 0)\n",
    "    return H\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_array(ex_name, f, exp_res, *args):\n",
    "    try:\n",
    "        res = f(*args)\n",
    "    except NotImplementedError:\n",
    "        log(red(\"FAIL\"), ex_name, \": not implemented\")\n",
    "        return True\n",
    "    if not type(res) == np.ndarray:\n",
    "        log(red(\"FAIL\"), ex_name, \": does not return a numpy array, type: \", type(res))\n",
    "        return True\n",
    "    if not len(res) == len(exp_res):\n",
    "        log(red(\"FAIL\"), ex_name, \": expected an array of shape \", exp_res.shape, \" but got array of shape\", res.shape)\n",
    "        return True\n",
    "    if not equals(res, exp_res):\n",
    "        log(red(\"FAIL\"), ex_name, \": incorrect answer. Expected\", exp_res, \", got: \", res)\n",
    "\n",
    "        return True\n",
    "\n",
    "def check_real(ex_name, f, exp_res, *args):\n",
    "    try:\n",
    "        res = f(*args)\n",
    "    except NotImplementedError:\n",
    "        log(red(\"FAIL\"), ex_name, \": not implemented\")\n",
    "        return True\n",
    "    if not np.isreal(res):\n",
    "        log(red(\"FAIL\"), ex_name, \": does not return a real number, type: \", type(res))\n",
    "        return True\n",
    "    if not -epsilon < res - exp_res < epsilon:\n",
    "        log(red(\"FAIL\"), ex_name, \": incorrect answer. Expected\", exp_res, \", got: \", res)\n",
    "        return True\n",
    "    \n",
    "def check_tuple(ex_name, f, exp_res, *args, **kwargs):\n",
    "    try:\n",
    "        res = f(*args, **kwargs)\n",
    "    except NotImplementedError:\n",
    "        log(red(\"FAIL\"), ex_name, \": not implemented\")\n",
    "        return True\n",
    "    if not type(res) == tuple:\n",
    "        log(red(\"FAIL\"), ex_name, \": does not return a tuple, type: \", type(res))\n",
    "        return True\n",
    "    if not len(res) == len(exp_res):\n",
    "        log(red(\"FAIL\"), ex_name, \": expected a tuple of size \", len(exp_res), \" but got tuple of size\", len(res))\n",
    "        return True\n",
    "    if not all(equals(x, y) for x, y in zip(res, exp_res)):\n",
    "        log(red(\"FAIL\"), ex_name, \": incorrect answer. Expected\", exp_res, \", got: \", res)\n",
    "        return True\n",
    "\n",
    "def get_classification(X, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Makes predictions by classifying a given dataset\n",
    "\n",
    "    Args:\n",
    "        X - (n, d - 1) NumPy array (n data points, each with d - 1 features)\n",
    "        theta - (k, d) NumPy array where row j represents the parameters of our model for\n",
    "                label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        Y - (n, ) NumPy array, containing the predicted label (a number between 0-9) for\n",
    "            each data point\n",
    "    \"\"\"\n",
    "    X = augment_feature_vector(X)\n",
    "    probabilities = compute_probabilities(X, theta, temp_parameter)\n",
    "    return np.argmax(probabilities, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_compute_probabilities():\n",
    "    ex_name = \"Compute probabilities\"\n",
    "    n, d, k = 3, 5, 7\n",
    "    X = np.arange(0, n * d).reshape(n, d)\n",
    "    zeros = np.zeros((k, d))\n",
    "    temp = 0.2\n",
    "    \n",
    "    exp_res = np.ones((k, n)) / k\n",
    "    if check_array(\n",
    "            ex_name, compute_probabilities,\n",
    "            exp_res, X, zeros, temp):\n",
    "        return\n",
    "\n",
    "    theta = np.arange(0, k * d).reshape(k, d)\n",
    "    compute_probabilities(X, theta, temp)\n",
    "    exp_res = np.zeros((k, n))\n",
    "    exp_res[-1] = 1\n",
    "    if check_array(\n",
    "            ex_name, compute_probabilities,\n",
    "            exp_res, X, theta, temp):\n",
    "        return\n",
    "\n",
    "    log(green(\"PASS\"), ex_name, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def green(s):\n",
    "    return '\\033[1;32m%s\\033[m' % s\n",
    "\n",
    "def yellow(s):\n",
    "    return '\\033[1;33m%s\\033[m' % s\n",
    "\n",
    "def red(s):\n",
    "    return '\\033[1;31m%s\\033[m' % s\n",
    "\n",
    "def log(*m):\n",
    "    print(\" \".join(map(str, m)))\n",
    "\n",
    "def log_exit(*m):\n",
    "    log(red(\"ERROR:\"), *m)\n",
    "    exit(1)\n",
    "    \n",
    "verbose = False\n",
    "\n",
    "epsilon = 1e-6\n",
    "\n",
    "def equals(x, y):\n",
    "    if type(y) == np.ndarray:\n",
    "        return (np.abs(x - y) < epsilon).all()\n",
    "    return -epsilon < x - y < epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mPASS\u001b[m Compute probabilities \n"
     ]
    }
   ],
   "source": [
    "test = check_compute_probabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(X, Y, theta, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes the total cost over every datapoint.\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns\n",
    "        c - the cost value (scalar)\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    #     exponent variable\n",
    "    probability_matrix = compute_probabilities(X, theta, temp_parameter)\n",
    "    log_param = np.log(probability_matrix/np.sum(probability_matrix, axis = 0))\n",
    "#     number of labels\n",
    "    k = theta.shape[0]\n",
    "#     number of datapoints\n",
    "    n = Y.shape[0]\n",
    "    regul_param = lambda_factor/2 * np.linalg.norm(theta)**2\n",
    "# creating Y matrix of size k x n and checking if y(i) == j    \n",
    "    Y_matrix = np.zeros((k, n))\n",
    "    for i in range(k):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                Y_matrix[i][j] = 1\n",
    "            else:\n",
    "                Y_matrix[i][j] = 0\n",
    "    error = (-1/n) * np.sum(log_param[Y_matrix == 1])\n",
    "    loss_function = error + regul_param\n",
    "    return loss_function\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function_2(X, Y, theta, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes the total cost over every datapoint.\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "    Returns\n",
    "        c - the cost value (scalar)\n",
    "    \"\"\"\n",
    "    import scipy.sparse as sparse\n",
    "    \n",
    "    # Get number of labels\n",
    "    k = theta.shape[0]\n",
    "    \n",
    "    # Get number of examples\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # avg error term\n",
    "    \n",
    "    # Clip prob matrix to avoid NaN instances\n",
    "    clip_prob_matrix = np.clip(compute_probabilities(X, theta, temp_parameter), 1e-15, 1-1e-15)\n",
    "    print(\"clip probability matrix: \", clip_prob_matrix)\n",
    "    \n",
    "    # Take the log of the matrix of probabilities\n",
    "    log_clip_matrix = np.log(clip_prob_matrix)\n",
    "    print(\"log value: \", log_clip_matrix)\n",
    "    \n",
    "    # Create a sparse matrix of [[y(i) == j]]\n",
    "    M = sparse.coo_matrix(([1]*n, (Y, range(n))), shape = (k,n)).toarray()\n",
    "    print(\"M: \", M)\n",
    "    \n",
    "    # Only add terms of log(matrix of prob) where M == 1\n",
    "    error_term = (-1/n)*np.sum(log_clip_matrix[M == 1])    \n",
    "    print(\"error_term: \", error_term )\n",
    "                \n",
    "    # Regularization term\n",
    "    reg_term = (lambda_factor/2)*np.linalg.norm(theta)**2\n",
    "    print(\"regul term: \", reg_term)\n",
    "    \n",
    "    return error_term + reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_compute_cost_function():\n",
    "    ex_name = \"Compute cost function\"\n",
    "    n, d, k = 3, 5, 7\n",
    "    X = np.arange(0, n * d).reshape(n, d)\n",
    "    Y = np.arange(0, n)\n",
    "    zeros = np.zeros((k, d))\n",
    "    temp = 0.2\n",
    "    lambda_factor = 0.5\n",
    "    exp_res = 1.9459101490553135\n",
    "    if check_real(\n",
    "            ex_name, compute_cost_function,\n",
    "            exp_res, X, Y, zeros, lambda_factor, temp):\n",
    "        return\n",
    "    log(green(\"PASS\"), ex_name, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_compute_cost_function_2():\n",
    "    ex_name = \"Compute cost function\"\n",
    "    n, d, k = 3, 5, 7\n",
    "    X = np.arange(0, n * d).reshape(n, d)\n",
    "    Y = np.arange(0, n)\n",
    "    zeros = np.zeros((k, d))\n",
    "    temp = 0.2\n",
    "    lambda_factor = 0.5\n",
    "    exp_res = 1.9459101490553135\n",
    "    if check_real(\n",
    "            ex_name, compute_cost_function_2,\n",
    "            exp_res, X, Y, zeros, lambda_factor, temp):\n",
    "        return\n",
    "    log(green(\"PASS\"), ex_name, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mFAIL\u001b[m Compute cost function : incorrect answer. Expected 1.9459101490553135 , got:  -5.8377304471659395\n"
     ]
    }
   ],
   "source": [
    "check_compute_cost_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip probability matrix:  [[0.14285714 0.14285714 0.14285714]\n",
      " [0.14285714 0.14285714 0.14285714]\n",
      " [0.14285714 0.14285714 0.14285714]\n",
      " [0.14285714 0.14285714 0.14285714]\n",
      " [0.14285714 0.14285714 0.14285714]\n",
      " [0.14285714 0.14285714 0.14285714]\n",
      " [0.14285714 0.14285714 0.14285714]]\n",
      "log value:  [[-1.94591015 -1.94591015 -1.94591015]\n",
      " [-1.94591015 -1.94591015 -1.94591015]\n",
      " [-1.94591015 -1.94591015 -1.94591015]\n",
      " [-1.94591015 -1.94591015 -1.94591015]\n",
      " [-1.94591015 -1.94591015 -1.94591015]\n",
      " [-1.94591015 -1.94591015 -1.94591015]\n",
      " [-1.94591015 -1.94591015 -1.94591015]]\n",
      "M:  [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "error_term:  1.9459101490553135\n",
      "regul term:  0.0\n",
      "\u001b[1;32mPASS\u001b[m Compute cost function \n"
     ]
    }
   ],
   "source": [
    "check_compute_cost_function_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_descent_iteration(X, Y, theta, alpha, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Runs one step of batch gradient descent\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        alpha - the learning rate (scalar)\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        theta - (k, d) NumPy array that is the final value of parameters theta\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    import scipy.sparse as sparse\n",
    "    import sys\n",
    "    eps = sys.float_info.epsilon\n",
    "#     number of datapoints\n",
    "    n = X.shape[0]\n",
    "#     number of labels\n",
    "    k = theta.shape[0]\n",
    "    \n",
    "#     Creat Y_matrix with 1s for each data point that is equal to j\n",
    "    import sys \n",
    "    Y_matrix = sparse.coo_matrix(([1]*n, (Y, range(n))), shape = (k,n)).toarray()\n",
    "    \n",
    "    prob_matrix = np.clip(compute_probabilities(X, theta, temp_parameter), eps, 1-eps)\n",
    "\n",
    "   \n",
    "    reg_param = lambda_factor * theta\n",
    "    theta_gradient = (-1/(temp_parameter*n))*np.matmul(Y_matrix - prob_matrix, X) + reg_param\n",
    "    optimized_theta = theta - alpha * theta_gradient\n",
    "    return optimized_theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_run_gradient_descent_iteration():\n",
    "    ex_name = \"Run gradient descent iteration\"\n",
    "    n, d, k = 3, 5, 7\n",
    "    X = np.arange(0, n * d).reshape(n, d)\n",
    "    Y = np.arange(0, n)\n",
    "    zeros = np.zeros((k, d))\n",
    "    alpha = 2\n",
    "    temp = 0.2\n",
    "    lambda_factor = 0.5\n",
    "    exp_res = np.zeros((k, d))\n",
    "    exp_res = np.array([\n",
    "       [ -7.14285714,  -5.23809524,  -3.33333333,  -1.42857143, 0.47619048],\n",
    "       [  9.52380952,  11.42857143,  13.33333333,  15.23809524, 17.14285714],\n",
    "       [ 26.19047619,  28.0952381 ,  30.        ,  31.9047619 , 33.80952381],\n",
    "       [ -7.14285714,  -8.57142857, -10.        , -11.42857143, -12.85714286],\n",
    "       [ -7.14285714,  -8.57142857, -10.        , -11.42857143, -12.85714286],\n",
    "       [ -7.14285714,  -8.57142857, -10.        , -11.42857143, -12.85714286],\n",
    "       [ -7.14285714,  -8.57142857, -10.        , -11.42857143, -12.85714286]\n",
    "    ])\n",
    "\n",
    "    if check_array(\n",
    "            ex_name, run_gradient_descent_iteration,\n",
    "            exp_res, X, Y, zeros, alpha, lambda_factor, temp):\n",
    "        return\n",
    "    run_gradient_descent_iteration(X, Y, zeros, alpha, lambda_factor, temp)\n",
    "    log(green(\"PASS\"), ex_name, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mPASS\u001b[m Run gradient descent iteration \n"
     ]
    }
   ],
   "source": [
    "check_run_gradient_descent_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_y(train_y, test_y):\n",
    "    \"\"\"\n",
    "    Changes the old digit labels for the training and test set for the new (mod 3)\n",
    "    labels.\n",
    "\n",
    "    Args:\n",
    "        train_y - (n, ) NumPy array containing the labels (a number between 0-9)\n",
    "                 for each datapoint in the training set\n",
    "        test_y - (n, ) NumPy array containing the labels (a number between 0-9)\n",
    "                for each datapoint in the test set\n",
    "\n",
    "    Returns:\n",
    "        train_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)\n",
    "                     for each datapoint in the training set\n",
    "        test_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)\n",
    "                    for each datapoint in the test set\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    train_y_mod3 = np.mod(train_y, 3)\n",
    "    test_y_mod3 = np.mod(test_y, 3)\n",
    "    print(\"train_y: \", train_y, \"\\ntrain_y_mod: \", test_y_mod3, \"\\ntest_y: \", test_y, \"\\ntest_y_mod: \", test_y_mod3)\n",
    "    return (train_y_mod3, test_y_mod3)\n",
    "    \n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_update_y():\n",
    "    ex_name = \"Update y\"\n",
    "    train_y = np.arange(0, 10)\n",
    "    test_y = np.arange(9, -1, -1)\n",
    "    exp_res = (\n",
    "            np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0]),\n",
    "            np.array([0, 2, 1, 0, 2, 1, 0, 2, 1, 0])\n",
    "            )\n",
    "    if check_tuple(\n",
    "            ex_name, update_y,\n",
    "            exp_res, train_y, test_y):\n",
    "        return\n",
    "    log(green(\"PASS\"), ex_name, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y:  [0 1 2 3 4 5 6 7 8 9] \n",
      "train_y_mod:  [0 2 1 0 2 1 0 2 1 0] \n",
      "test_y:  [9 8 7 6 5 4 3 2 1 0] \n",
      "test_y_mod:  [0 2 1 0 2 1 0 2 1 0]\n",
      "\u001b[1;32mPASS\u001b[m Update y \n"
     ]
    }
   ],
   "source": [
    "check_update_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_error_mod3(X, Y, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Returns the error of these new labels when the classifier predicts the digit. (mod 3)\n",
    "\n",
    "    Args:\n",
    "        X - (n, d - 1) NumPy array (n datapoints each with d - 1 features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-2) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        test_error - the error rate of the classifier (scalar)\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    Y_pred = get_classification(X, theta, temp_parameter)\n",
    "    return 1- (np.mod(Y_pred, 3) == Y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def center_data(X):\n",
    "    \"\"\"\n",
    "    Returns a centered version of the data, where each feature now has mean = 0\n",
    "\n",
    "    Args:\n",
    "        X - n x d NumPy array of n data points, each with d features\n",
    "\n",
    "    Returns:\n",
    "        - (n, d) NumPy array X' where for each i = 1, ..., n and j = 1, ..., d:\n",
    "        X'[i][j] = X[i][j] - means[j]       \n",
    "\t- (d, ) NumPy array with the columns means\n",
    "\n",
    "    \"\"\"\n",
    "    feature_means = X.mean(axis=0)\n",
    "    return (X - feature_means), feature_means\n",
    "\n",
    "def principal_components(centered_data):\n",
    "    \"\"\"\n",
    "    Returns the principal component vectors of the data, sorted in decreasing order\n",
    "    of eigenvalue magnitude. This function first calculates the covariance matrix\n",
    "    and then finds its eigenvectors.\n",
    "\n",
    "    Args:\n",
    "        centered_data - n x d NumPy array of n data points, each with d features\n",
    "\n",
    "    Returns:\n",
    "        d x d NumPy array whose columns are the principal component directions sorted\n",
    "        in descending order by the amount of variation each direction (these are\n",
    "        equivalent to the d eigenvectors of the covariance matrix sorted in descending\n",
    "        order of eigenvalues, so the first column corresponds to the eigenvector with\n",
    "        the largest eigenvalue\n",
    "    \"\"\"\n",
    "    scatter_matrix = np.dot(centered_data.transpose(), centered_data)\n",
    "    eigen_values, eigen_vectors = np.linalg.eig(scatter_matrix)\n",
    "    # Re-order eigenvectors by eigenvalue magnitude:\n",
    "    idx = eigen_values.argsort()[::-1]\n",
    "    eigen_values = eigen_values[idx]\n",
    "    eigen_vectors = eigen_vectors[:, idx]\n",
    "    return eigen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_PC(X, pcs, n_components, feature_means):\n",
    "    \"\"\"\n",
    "    Given principal component vectors pcs = principal_components(X)\n",
    "    this function returns a new data array in which each sample in X\n",
    "    has been projected onto the first n_components principcal components.\n",
    "    \"\"\"\n",
    "    # TODO: first center data using the feature_means\n",
    "    # TODO: Return the projection of the centered dataset\n",
    "    #       on the first n_components principal components.\n",
    "    #       This should be an array with dimensions: n x n_components.\n",
    "    # Hint: these principal components = first n_components columns\n",
    "    #       of the eigenvectors returned by principal_components().\n",
    "    #       Note that each eigenvector is already be a unit-vector,\n",
    "    #       so the projection may be done using matrix multiplication.\n",
    "    print(\"X: \", X, \"\\npcs: \", pcs, \"\\nn_components: \", n_components, \"\\nfeature means: \", feature_means)\n",
    "#     centered_x = center_data(X)\n",
    "    centered_x = (X - feature_means), feature_means\n",
    "    first_n_components =pcs [:,:n_components]\n",
    "#     print(\"first n components: \", first_n_components)\n",
    "    project_of_X = np.matmul(centered_x[0], first_n_components)\n",
    "    print(\"Project on X: \", project_of_X)\n",
    "    return project_of_X\n",
    "#     print(\"project of X: \", project_of_X)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_project_onto_PC():\n",
    "    ex_name = \"Project onto PC\"\n",
    "    X = np.array([\n",
    "        [1, 2, 3],\n",
    "        [2, 4, 6],\n",
    "        [3, 6, 9],\n",
    "        [4, 8, 12],\n",
    "    ]);\n",
    "    x_centered, feature_means = center_data(X)\n",
    "    pcs = principal_components(x_centered)\n",
    "    exp_res = np.array([\n",
    "        [5.61248608, 0],\n",
    "        [1.87082869, 0],\n",
    "        [-1.87082869, 0],\n",
    "        [-5.61248608, 0],\n",
    "    ])\n",
    "    n_components = 2\n",
    "    if check_array(\n",
    "            ex_name, project_onto_PC,\n",
    "            exp_res, X, pcs, n_components, feature_means):\n",
    "        return\n",
    "    log(green(\"PASS\"), ex_name, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[ 1  2  3]\n",
      " [ 2  4  6]\n",
      " [ 3  6  9]\n",
      " [ 4  8 12]] \n",
      "pcs:  [[-2.67261242e-01 -8.99989016e-01 -2.59226735e-16]\n",
      " [-5.34522484e-01 -1.58890294e-01 -8.32050294e-01]\n",
      " [-8.01783726e-01  4.05923201e-01  5.54700196e-01]] \n",
      "n_components:  2 \n",
      "feature means:  [2.5 5.  7.5]\n",
      "Project on X:  [[ 5.61248608e+00 -1.22124533e-15]\n",
      " [ 1.87082869e+00 -4.44089210e-16]\n",
      " [-1.87082869e+00  4.44089210e-16]\n",
      " [-5.61248608e+00  1.22124533e-15]]\n",
      "\u001b[1;32mPASS\u001b[m Project onto PC \n"
     ]
    }
   ],
   "source": [
    "check_project_onto_PC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  np.array([[0.92659394, 0.06660378, 0.64196431, 0.27696162, 0.72337484, 0.90830364,\n",
    "  0.40062915, 0.82774709, 0.52137306, 0.74789201],\n",
    " [0.7427347,  0.10113444, 0.81709577, 0.20700801, 0.9105606,  0.60936657,\n",
    "  0.2922968,  0.55525197, 0.27937088, 0.84093742],\n",
    " [0.79646569, 0.40015527, 0.73578419, 0.50150713, 0.08213383, 0.13735689,\n",
    "  0.33527088, 0.56752578, 0.16147387, 0.83859926],\n",
    " [0.72613046, 0.22787194, 0.18091829, 0.19583052, 0.28202112, 0.06734956,\n",
    "  0.00110204, 0.9582537,  0.88191493, 0.3163989 ],\n",
    " [0.26178795, 0.41712771, 0.68889106, 0.54785258, 0.00361445, 0.46121479,\n",
    "  0.95232254, 0.16778654, 0.92671663, 0.47733262],\n",
    " [0.7390662, 0.94973171, 0.22410595, 0.97580493, 0.05287743, 0.4915418,\n",
    "  0.64024163, 0.82175735, 0.22797431, 0.63175989],\n",
    " [0.28034884, 0.16075991, 0.89564791, 0.07715863, 0.62344849, 0.84620427,\n",
    "  0.29967612, 0.08008401, 0.86131521, 0.61931149],\n",
    " [0.1931245,  0.64425872, 0.33031285, 0.20682162, 0.98687352, 0.99500653,\n",
    "  0.53019341, 0.35579074, 0.96908464, 0.29741462],\n",
    " [0.47221524, 0.80250748, 0.48130075, 0.27456139, 0.92659659, 0.5519729,\n",
    "  0.07137089, 0.15351087, 0.66202216, 0.71958692],\n",
    " [0.28373956, 0.08931194, 0.53522626, 0.59798178, 0.64485229, 0.61733186,\n",
    "  0.56826081, 0.15266871, 0.34187707, 0.79326544]]) \n",
    "pcs =  np.array([[-0.30429069,  0.43839785, -0.02850306,  0.08741634,  0.15186944, -0.38242382,\n",
    "  -0.08456488, -0.49582227,  0.21789731, -0.48680383],\n",
    " [-0.16108074, -0.38332457, -0.25580848,  0.58059012, -0.35213761, -0.5085185,\n",
    "  -0.15350204,  0.09154921,  0.00531509,  0.10014017],\n",
    " [ 0.1610541,   0.11905386,  0.46643123, -0.27054336, -0.06310146, -0.56213225,\n",
    "  -0.19063877,  0.21184606, -0.51411515, -0.06020971],\n",
    " [-0.35250003, -0.27420894,  0.22331222,  0.27424407,  0.08284565,  0.28679188,\n",
    "   0.24338107, -0.29815032, -0.62794704, -0.2095629 ],\n",
    " [ 0.56430656,  0.33141264, -0.07685699,  0.46931908, -0.03955667,  0.26203338,\n",
    "  -0.43327335, -0.15923849, -0.2433229,  -0.05514908],\n",
    " [ 0.40388805, -0.0520969,   0.13035549,  0.36349793,  0.53930041, -0.26038388,\n",
    "   0.54266376,  0.12861014,  0.11663913, -0.04198996],\n",
    " [-0.05984081, -0.44992898,  0.32791156,  0.01223199,  0.50891766,  0.03015108,\n",
    "  -0.59248815, -0.12000641,  0.23357575,  0.07999836],\n",
    " [-0.38205651,  0.35991297, -0.35465042,  0.08187256,  0.48921962, -0.05401186,\n",
    "  -0.12456861,  0.25103886, -0.30220792,  0.42414013],\n",
    " [ 0.31291674, -0.28613341, -0.48464091, -0.36152662,  0.13456998, -0.22813058,\n",
    "   0.04922592, -0.54247048, -0.2458977,   0.16527775],\n",
    " [-0.04725513,  0.21239361,  0.41862141,  0.13009456, -0.18231245, -0.06773125,\n",
    "   0.1500531,  -0.44543564,  0.12123112,  0.69787983]]) \n",
    "n_components =  1 \n",
    "feature_means = np.array([0.54222071, 0.38594629, 0.55312473, 0.38614882, 0.52363531, 0.56856488,\n",
    " 0.40913643, 0.46403768, 0.58331228, 0.62824986])\n",
    "first_n_components =  np.array([[-0.30429069],\n",
    " [-0.16108074],\n",
    " [ 0.1610541 ],\n",
    " [-0.35250003],\n",
    " [ 0.56430656],\n",
    " [ 0.40388805],\n",
    " [-0.05984081],\n",
    " [-0.38205651],\n",
    " [ 0.31291674],\n",
    " [-0.04725513]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[0.92659394 0.06660378 0.64196431 0.27696162 0.72337484 0.90830364\n",
      "  0.40062915 0.82774709 0.52137306 0.74789201]\n",
      " [0.7427347  0.10113444 0.81709577 0.20700801 0.9105606  0.60936657\n",
      "  0.2922968  0.55525197 0.27937088 0.84093742]\n",
      " [0.79646569 0.40015527 0.73578419 0.50150713 0.08213383 0.13735689\n",
      "  0.33527088 0.56752578 0.16147387 0.83859926]\n",
      " [0.72613046 0.22787194 0.18091829 0.19583052 0.28202112 0.06734956\n",
      "  0.00110204 0.9582537  0.88191493 0.3163989 ]\n",
      " [0.26178795 0.41712771 0.68889106 0.54785258 0.00361445 0.46121479\n",
      "  0.95232254 0.16778654 0.92671663 0.47733262]\n",
      " [0.7390662  0.94973171 0.22410595 0.97580493 0.05287743 0.4915418\n",
      "  0.64024163 0.82175735 0.22797431 0.63175989]\n",
      " [0.28034884 0.16075991 0.89564791 0.07715863 0.62344849 0.84620427\n",
      "  0.29967612 0.08008401 0.86131521 0.61931149]\n",
      " [0.1931245  0.64425872 0.33031285 0.20682162 0.98687352 0.99500653\n",
      "  0.53019341 0.35579074 0.96908464 0.29741462]\n",
      " [0.47221524 0.80250748 0.48130075 0.27456139 0.92659659 0.5519729\n",
      "  0.07137089 0.15351087 0.66202216 0.71958692]\n",
      " [0.28373956 0.08931194 0.53522626 0.59798178 0.64485229 0.61733186\n",
      "  0.56826081 0.15266871 0.34187707 0.79326544]] \n",
      "pcs:  [[-0.30429069  0.43839785 -0.02850306  0.08741634  0.15186944 -0.38242382\n",
      "  -0.08456488 -0.49582227  0.21789731 -0.48680383]\n",
      " [-0.16108074 -0.38332457 -0.25580848  0.58059012 -0.35213761 -0.5085185\n",
      "  -0.15350204  0.09154921  0.00531509  0.10014017]\n",
      " [ 0.1610541   0.11905386  0.46643123 -0.27054336 -0.06310146 -0.56213225\n",
      "  -0.19063877  0.21184606 -0.51411515 -0.06020971]\n",
      " [-0.35250003 -0.27420894  0.22331222  0.27424407  0.08284565  0.28679188\n",
      "   0.24338107 -0.29815032 -0.62794704 -0.2095629 ]\n",
      " [ 0.56430656  0.33141264 -0.07685699  0.46931908 -0.03955667  0.26203338\n",
      "  -0.43327335 -0.15923849 -0.2433229  -0.05514908]\n",
      " [ 0.40388805 -0.0520969   0.13035549  0.36349793  0.53930041 -0.26038388\n",
      "   0.54266376  0.12861014  0.11663913 -0.04198996]\n",
      " [-0.05984081 -0.44992898  0.32791156  0.01223199  0.50891766  0.03015108\n",
      "  -0.59248815 -0.12000641  0.23357575  0.07999836]\n",
      " [-0.38205651  0.35991297 -0.35465042  0.08187256  0.48921962 -0.05401186\n",
      "  -0.12456861  0.25103886 -0.30220792  0.42414013]\n",
      " [ 0.31291674 -0.28613341 -0.48464091 -0.36152662  0.13456998 -0.22813058\n",
      "   0.04922592 -0.54247048 -0.2458977   0.16527775]\n",
      " [-0.04725513  0.21239361  0.41862141  0.13009456 -0.18231245 -0.06773125\n",
      "   0.1500531  -0.44543564  0.12123112  0.69787983]] \n",
      "n_components:  1 \n",
      "feature means:  [0.54222071 0.38594629 0.55312473 0.38614882 0.52363531 0.56856488\n",
      " 0.40913643 0.46403768 0.58331228 0.62824986]\n",
      "Project on X:  [[ 0.07372197]\n",
      " [ 0.19233155]\n",
      " [-0.69125938]\n",
      " [-0.41836382]\n",
      " [-0.09636454]\n",
      " [-0.97017302]\n",
      " [ 0.68915899]\n",
      " [ 0.69604928]\n",
      " [ 0.36182589]\n",
      " [ 0.16307314]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.07372197],\n",
       "       [ 0.19233155],\n",
       "       [-0.69125938],\n",
       "       [-0.41836382],\n",
       "       [-0.09636454],\n",
       "       [-0.97017302],\n",
       "       [ 0.68915899],\n",
       "       [ 0.69604928],\n",
       "       [ 0.36182589],\n",
       "       [ 0.16307314]])"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_onto_PC(X, pcs, n_components, feature_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubic feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_features(X):\n",
    "    \"\"\"\n",
    "    Returns a new dataset with features given by the mapping\n",
    "    which corresponds to the cubic kernel.\n",
    "    \"\"\"\n",
    "    n, d = X.shape  # dataset size, input dimension\n",
    "    X_withones = np.ones((n, d + 1))\n",
    "    X_withones[:, :-1] = X\n",
    "    new_d = 0  # dimension of output\n",
    "    new_d = int((d + 1) * (d + 2) * (d + 3) / 6)\n",
    "\n",
    "    new_data = np.zeros((n, n ew_d))\n",
    "    col_index = 0\n",
    "    for x_i in range(n):\n",
    "        X_i = X[x_i]\n",
    "        X_i = X_i.reshape(1, X_i.size)\n",
    "\n",
    "        if d > 2:\n",
    "            comb_2 = np.matmul(np.transpose(X_i), X_i)\n",
    "\n",
    "            unique_2 = comb_2[np.triu_indices(d, 1)]\n",
    "            unique_2 = unique_2.reshape(unique_2.size, 1)\n",
    "            comb_3 = np.matmul(unique_2, X_i)\n",
    "            keep_m = np.zeros(comb_3.shape)\n",
    "            index = 0\n",
    "            for i in range(d - 1):\n",
    "                keep_m[index + np.arange(d - 1 - i), i] = 0\n",
    "\n",
    "                tri_keep = np.triu_indices(d - 1 - i, 1)\n",
    "\n",
    "                correct_0 = tri_keep[0] + index\n",
    "                correct_1 = tri_keep[1] + i + 1\n",
    "\n",
    "                keep_m[correct_0, correct_1] = 1\n",
    "                index += d - 1 - i\n",
    "\n",
    "            unique_3 = np.sqrt(6) * comb_3[np.nonzero(keep_m)]\n",
    "\n",
    "            new_data[x_i, np.arange(unique_3.size)] = unique_3\n",
    "            col_index = unique_3.size\n",
    "\n",
    "    for i in range(n):\n",
    "        newdata_colindex = col_index\n",
    "        for j in range(d + 1):\n",
    "            new_data[i, newdata_colindex] = X_withones[i, j]**3\n",
    "            newdata_colindex += 1\n",
    "            for k in range(j + 1, d + 1):\n",
    "                new_data[i, newdata_colindex] = X_withones[i, j]**2 * X_withones[i, k] * (3**(0.5))\n",
    "                newdata_colindex += 1\n",
    "\n",
    "                new_data[i, newdata_colindex] = X_withones[i, j] * X_withones[i, k]**2 * (3**(0.5))\n",
    "                newdata_colindex += 1\n",
    "\n",
    "                if k < d:\n",
    "                    new_data[i, newdata_colindex] = X_withones[i, j] * X_withones[i, k] * (6**(0.5))\n",
    "                    newdata_colindex += 1\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "        [1, 2, 3],\n",
    "        [2, 4, 6],\n",
    "        [3, 6, 9],\n",
    "        [4, 8, 12],\n",
    "    ]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.46969385e+01, 1.00000000e+00, 3.46410162e+00, 6.92820323e+00,\n",
       "        4.89897949e+00, 5.19615242e+00, 1.55884573e+01, 7.34846923e+00,\n",
       "        1.73205081e+00, 1.73205081e+00, 8.00000000e+00, 2.07846097e+01,\n",
       "        3.11769145e+01, 1.46969385e+01, 6.92820323e+00, 3.46410162e+00,\n",
       "        2.70000000e+01, 1.55884573e+01, 5.19615242e+00, 1.00000000e+00],\n",
       "       [1.17575508e+02, 8.00000000e+00, 2.77128129e+01, 5.54256258e+01,\n",
       "        1.95959179e+01, 4.15692194e+01, 1.24707658e+02, 2.93938769e+01,\n",
       "        6.92820323e+00, 3.46410162e+00, 6.40000000e+01, 1.66276878e+02,\n",
       "        2.49415316e+02, 5.87877538e+01, 2.77128129e+01, 6.92820323e+00,\n",
       "        2.16000000e+02, 6.23538291e+01, 1.03923048e+01, 1.00000000e+00],\n",
       "       [3.96817338e+02, 2.70000000e+01, 9.35307436e+01, 1.87061487e+02,\n",
       "        4.40908154e+01, 1.40296115e+02, 4.20888346e+02, 6.61362231e+01,\n",
       "        1.55884573e+01, 5.19615242e+00, 2.16000000e+02, 5.61184462e+02,\n",
       "        8.41776692e+02, 1.32272446e+02, 6.23538291e+01, 1.03923048e+01,\n",
       "        7.29000000e+02, 1.40296115e+02, 1.55884573e+01, 1.00000000e+00],\n",
       "       [9.40604061e+02, 6.40000000e+01, 2.21702503e+02, 4.43405007e+02,\n",
       "        7.83836718e+01, 3.32553755e+02, 9.97661265e+02, 1.17575508e+02,\n",
       "        2.77128129e+01, 6.92820323e+00, 5.12000000e+02, 1.33021502e+03,\n",
       "        1.99532253e+03, 2.35151015e+02, 1.10851252e+02, 1.38564065e+01,\n",
       "        1.72800000e+03, 2.49415316e+02, 2.07846097e+01, 1.00000000e+00]])"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cubic_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(X, Y, c, p):\n",
    "    \"\"\"\n",
    "        Compute the polynomial kernel between two matrices X and Y::\n",
    "            K(x, y) = (<x, y> + c)^p\n",
    "        for each pair of rows x in X and y in Y.\n",
    "\n",
    "        Args:\n",
    "            X - (n, d) NumPy array (n datapoints each with d features)\n",
    "            Y - (m, d) NumPy array (m datapoints each with d features)\n",
    "            c - a coefficient to trade off high-order and low-order terms (scalar)\n",
    "            p - the degree of the polynomial kernel\n",
    "\n",
    "        Returns:\n",
    "            kernel_matrix - (n, m) Numpy array containing the kernel matrix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    kernel = (X @ Y.T + c) ** p\n",
    "\n",
    "    return kernel\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_polynomial_kernel():\n",
    "    ex_name = \"Polynomial kernel\"\n",
    "    n, m, d = 3, 5, 7\n",
    "    c = 1\n",
    "    p = 2\n",
    "    X = np.random.random((n, d))\n",
    "    Y = np.random.random((m, d))\n",
    "    try:\n",
    "        K = polynomial_kernel(X, Y, c, d)\n",
    "    except NotImplementedError:\n",
    "        log(red(\"FAIL\"), ex_name, \": not implemented\")\n",
    "        return True\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            exp = (X[i] @ Y[j] + c) ** d\n",
    "            got = K[i][j]\n",
    "            if (not equals(exp, got)):\n",
    "                log(\n",
    "                    red(\"FAIL\"), ex_name,\n",
    "                    \": values at ({}, {}) do not match. Expected {}, got {}\"\n",
    "                    .format(i, j, exp, got)\n",
    "                )\n",
    "    log(green(\"PASS\"), ex_name, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mPASS\u001b[m Polynomial kernel \n"
     ]
    }
   ],
   "source": [
    "check_polynomial_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X, Y, gamma):\n",
    "    \"\"\"\n",
    "        Compute the Gaussian RBF kernel between two matrices X and Y::\n",
    "            K(x, y) = exp(-gamma ||x-y||^2)\n",
    "        for each pair of rows x in X and y in Y.\n",
    "\n",
    "        Args:\n",
    "            X - (n, d) NumPy array (n datapoints each with d features)\n",
    "            Y - (m, d) NumPy array (m datapoints each with d features)\n",
    "            gamma - the gamma parameter of gaussian function (scalar)\n",
    "\n",
    "        Returns:\n",
    "            kernel_matrix - (n, m) Numpy array containing the kernel matrix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    X_matrix = X\n",
    "    Y_matrix = Y\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    m = Y.shape[0]\n",
    "    if m > n:\n",
    "        X_matrix = np.ones([m,d])\n",
    "        print(\"X_matrix: \", X_matrix, \"\\nX: \", X)\n",
    "        X_matrix = np.matmul(X_matrix, X.T)\n",
    "        print(\"X_matrix: \", X_matrix, \"\\nX: \", X, \"\\nY_matrix: \", Y_matrix)\n",
    "        kernel = np.exp(-gamma * (np.linalg.norm(X_matrix - Y_matrix) ** 2))\n",
    "    else:\n",
    "        Y_matrix = np.ones([n,d])\n",
    "        print(\"Y_matrix: \", Y_matrix, \"\\nY: \", Y)\n",
    "        Y_matrix = np.matmul(Y_matrix, Y.T)\n",
    "        kernel = np.exp(-gamma * (np.linalg.norm(X_matrix - Y_matrix) ** 2))\n",
    "    \n",
    "    \n",
    "    print(\"X: \", X, \"\\nY: \", Y)\n",
    "    return kernel\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rbf_kernel():\n",
    "    ex_name = \"RBF kernel\"\n",
    "    n, m, d = 3, 5, 7\n",
    "    gamma = 0.5\n",
    "    X = np.random.random((n, d))\n",
    "    Y = np.random.random((m, d))\n",
    "    try:\n",
    "        K = rbf_kernel(X, Y, gamma)\n",
    "    except NotImplementedError:\n",
    "        log(red(\"FAIL\"), ex_name, \": not implemented\")\n",
    "        return True\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            exp = np.exp(-gamma * (np.linalg.norm(X[i] - Y[j]) ** 2))\n",
    "            got = K[i][j]\n",
    "            if (not equals(exp, got)):\n",
    "                log(\n",
    "                    red(\"FAIL\"), ex_name,\n",
    "                    \": values at ({}, {}) do not match. Expected {}, got {}\"\n",
    "                    .format(i, j, exp, got)\n",
    "                )\n",
    "    log(green(\"PASS\"), ex_name, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_matrix:  [[1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]] \n",
      "X:  [[0.60865983 0.844655   0.89936528 0.51632518 0.16722128 0.97525981\n",
      "  0.86629503]\n",
      " [0.20430173 0.44798527 0.3878968  0.77566654 0.560513   0.71716909\n",
      "  0.48707963]\n",
      " [0.8822927  0.54656844 0.2858943  0.2162454  0.63099602 0.31030631\n",
      "  0.00104014]]\n",
      "X_matrix:  [[4.8777814  3.58061205 2.87334331]\n",
      " [4.8777814  3.58061205 2.87334331]\n",
      " [4.8777814  3.58061205 2.87334331]\n",
      " [4.8777814  3.58061205 2.87334331]\n",
      " [4.8777814  3.58061205 2.87334331]] \n",
      "X:  [[0.60865983 0.844655   0.89936528 0.51632518 0.16722128 0.97525981\n",
      "  0.86629503]\n",
      " [0.20430173 0.44798527 0.3878968  0.77566654 0.560513   0.71716909\n",
      "  0.48707963]\n",
      " [0.8822927  0.54656844 0.2858943  0.2162454  0.63099602 0.31030631\n",
      "  0.00104014]] \n",
      "Y_matrix:  [[0.63891162 0.00694718 0.59951786 0.79926705 0.6881372  0.09639444\n",
      "  0.78720575]\n",
      " [0.11614702 0.18473289 0.25076959 0.08656907 0.1850281  0.13641093\n",
      "  0.96899474]\n",
      " [0.20960318 0.88104816 0.10148849 0.20824325 0.52947145 0.24365695\n",
      "  0.77310307]\n",
      " [0.1185583  0.6402109  0.1801005  0.25569593 0.84594987 0.99877419\n",
      "  0.00664322]\n",
      " [0.30950044 0.77429456 0.94214096 0.82503472 0.83229383 0.13891206\n",
      "  0.28316467]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,3) (5,7) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-608-b9c940593d93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcheck_rbf_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-607-3318fe7eedb9>\u001b[0m in \u001b[0;36mcheck_rbf_kernel\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrbf_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FAIL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\": not implemented\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-606-e8ced7b53445>\u001b[0m in \u001b[0;36mrbf_kernel\u001b[1;34m(X, Y, gamma)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mX_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X_matrix: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\nX: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\nY_matrix: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_matrix\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY_matrix\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mY_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,3) (5,7) "
     ]
    }
   ],
   "source": [
    "check_rbf_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:6.86x]",
   "language": "python",
   "name": "conda-env-6.86x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
